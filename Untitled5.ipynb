{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9610743e-96b9-475f-8649-edde2cb75d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Coba load pakai separator: ','\n",
      "‚úÖ Berhasil load!\n",
      "Kolom: ['Date\\tUser\\tTweet\\tsentiment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date\\tUser\\tTweet\\tsentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-03-31 14:32:04+00:00\\tpikobar_jabar\\tKetahui informasi pembagian #PPKM di wilayah Jabar berdasarkan level 3</th>\n",
       "      <td>2 dan 1 di #PikoData https://t.co/o2RnI7eDue\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31 09:26:00+00:00\\tinewsdotid\\t\"Tempat Ibadah di Wilayah PPKM Level 1 Boleh Berkapasitas 100 Persen. Baca Selengkapnya di https://t.co/JfIG6nIimN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#Ramadhan #PPKM #inews https://t.co/ky1G5xYLQB\"\\t1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Begini kira-kira gambaran bukber tanpa ngobrol. Bagaimana tanggapan sobat VDVC?</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#VDVCNews #Viral #PPKM #Bukber https://t.co/SWuFmfaIYN\"\\t1</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        Date\\tUser\\tTweet\\tsentiment\n",
       "2022-03-31 14:32:04+00:00\\tpikobar_jabar\\tKetah...   2 dan 1 di #PikoData https://t.co/o2RnI7eDue\\t1\n",
       "2022-03-31 09:26:00+00:00\\tinewsdotid\\t\"Tempat ...                                               NaN\n",
       "#Ramadhan #PPKM #inews https://t.co/ky1G5xYLQB\"\\t1                                               NaN\n",
       "Begini kira-kira gambaran bukber tanpa ngobrol....                                               NaN\n",
       "#VDVCNews #Viral #PPKM #Bukber https://t.co/SWu...                                               NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell X: Load Dataset Sosial Media (Auto-Detect) =====\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"INA_TweetsPPKM_Labeled_Pure.csv\"\n",
    "\n",
    "# List kemungkinan separator\n",
    "separators = [\",\", \";\", \"\\t\"]\n",
    "\n",
    "df = None\n",
    "for sep in separators:\n",
    "    try:\n",
    "        print(f\"üîç Coba load pakai separator: '{sep}'\")\n",
    "        df = pd.read_csv(file_path, sep=sep, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        print(\"‚úÖ Berhasil load!\")\n",
    "        print(\"Kolom:\", df.columns.tolist())\n",
    "        display(df.head())\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Gagal pakai separator '{sep}':\", e)\n",
    "\n",
    "if df is None:\n",
    "    print(\"‚ö†Ô∏è Semua cara gagal, coba pakai encoding lain (contoh: latin1).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45cacf78-af89-4d86-aa0d-f70ee6d38278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date\\tUser\\tTweet\\tsentiment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49277266-4a83-4565-aea2-ff6f438f2f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom dataset sosmed: ['Date', 'User', 'Tweet', 'sentiment']\n",
      "                        Date           User  \\\n",
      "0  2022-03-31 14:32:04+00:00  pikobar_jabar   \n",
      "1  2022-03-31 09:26:00+00:00     inewsdotid   \n",
      "2  2022-03-31 05:02:34+00:00      vdvc_talk   \n",
      "\n",
      "                                               Tweet  sentiment  \n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...          1  \n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...          1  \n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...          1  \n",
      "\n",
      "Preview dataset sosmed standar:\n",
      "                                                text  label   sumber\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...      0  Twitter\n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...      0  Twitter\n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...      0  Twitter\n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...      0  Twitter\n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...      0  Twitter\n",
      "\n",
      "Distribusi label:\n",
      "label\n",
      "0    23644\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: Standarisasi Dataset Sosmed =====\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset dengan pemisah TAB\n",
    "df = pd.read_csv(\"INA_TweetsPPKM_Labeled_Pure.csv\", sep=\"\\t\")\n",
    "\n",
    "print(\"Kolom dataset sosmed:\", df.columns.tolist())\n",
    "print(df.head(3))\n",
    "\n",
    "# Fungsi standarisasi\n",
    "def standardize_sosmed(df):\n",
    "    # Ambil teks dari kolom Tweet\n",
    "    if \"Tweet\" in df.columns:\n",
    "        teks = df[\"Tweet\"].fillna(\"\")\n",
    "    else:\n",
    "        raise ValueError(\"‚ö†Ô∏è Tidak ada kolom Tweet di dataset sosmed\")\n",
    "\n",
    "    # Ambil label (sementara pakai sentiment ‚Üí bisa diganti manual jadi hoax / real)\n",
    "    if \"sentiment\" in df.columns:\n",
    "        label = df[\"sentiment\"].apply(lambda x: 1 if str(x).lower() == \"negative\" else 0)\n",
    "    else:\n",
    "        label = \"unknown\"\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"text\": teks,\n",
    "        \"label\": label,\n",
    "        \"sumber\": \"Twitter\"\n",
    "    })\n",
    "\n",
    "# Terapkan standarisasi\n",
    "data_sosmed = standardize_sosmed(df)\n",
    "\n",
    "print(\"\\nPreview dataset sosmed standar:\")\n",
    "print(data_sosmed.head())\n",
    "print(\"\\nDistribusi label:\")\n",
    "print(data_sosmed[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d521d2-bf4d-4f0a-b66e-11caa207539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label otomatis:\n",
      "label\n",
      "0    23532\n",
      "1      112\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Contoh data hasil labeling:\n",
      "                                                text  label   sumber\n",
      "0  Ketahui informasi pembagian #PPKM di wilayah J...      0  Twitter\n",
      "1  Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...      0  Twitter\n",
      "2  Juru bicara Satgas Covid-19, Wiku Adisasmito m...      0  Twitter\n",
      "3  Ketahui informasi pembagian #PPKM di wilayah J...      0  Twitter\n",
      "4  Kementerian Agama menerbitkan Surat Edaran Nom...      0  Twitter\n",
      "5  Kapasitas tempat ibadah, termasuk masjid, yang...      0  Twitter\n",
      "6  Halo Sobat Sehat\\n\\nDengan adanya kondisi ini,...      0  Twitter\n",
      "7  Mitigasi Penting untuk Cegah Penyebaran Varian...      0  Twitter\n",
      "8  Sebanyak 5 (lima) kabupaten di Provinsi Lampun...      0  Twitter\n",
      "9  Komentar Satgas Penanganan Covid-19 Tentang Bu...      0  Twitter\n",
      "\n",
      "‚úÖ Dataset sosmed dengan kolom [text, label, sumber] berhasil disimpan ke 'sosmed_labeled_auto.csv'\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: Load & Semi-Otomatis Labeling Sosmed =====\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset sosmed\n",
    "df = pd.read_csv(\"INA_TweetsPPKM_Labeled_Pure.csv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "\n",
    "# Ambil kolom teks dari \"Tweet\"\n",
    "data_sosmed = pd.DataFrame({\n",
    "    \"text\": df[\"Tweet\"].fillna(\"\"),\n",
    "})\n",
    "\n",
    "# Keyword HOAX umum\n",
    "keywords_hoax = [\n",
    "    \"konspirasi\", \"tanpa bukti\", \"katanya\", \"isu hoax\", \n",
    "    \"obat ajaib\", \"sumber tidak jelas\", \"percaya atau tidak\",\n",
    "    \"hoax\", \"fitnah\", \"berita palsu\", \"provokasi\", \"isu palsu\"\n",
    "]\n",
    "\n",
    "# Keyword HOAX spesifik COVID/PPKM\n",
    "keywords_covid = [\n",
    "    \"vaksin bikin chip\", \"vaksin berbahaya\", \"vaksin mematikan\",\n",
    "    \"vaksin palsu\", \"vaksin haram\", \"ppkm konspirasi\",\n",
    "    \"lockdown konspirasi\", \"plandemi\", \"virus buatan\",\n",
    "    \"vaksin tidak perlu\", \"microchip\", \"5g penyebab covid\"\n",
    "]\n",
    "\n",
    "# Gabung semua keyword\n",
    "all_keywords = keywords_hoax + keywords_covid\n",
    "\n",
    "# Fungsi auto-labeling\n",
    "def label_rule(text):\n",
    "    text = str(text).lower()\n",
    "    for kw in all_keywords:\n",
    "        if kw in text:\n",
    "            return 1   # HOAX\n",
    "    return 0           # REAL\n",
    "\n",
    "# Terapkan fungsi labeling\n",
    "data_sosmed[\"label\"] = data_sosmed[\"text\"].apply(label_rule)\n",
    "data_sosmed[\"sumber\"] = \"Twitter\"\n",
    "\n",
    "# Pilih hanya kolom yang dibutuhkan\n",
    "data_sosmed_final = data_sosmed[[\"text\", \"label\", \"sumber\"]]\n",
    "\n",
    "# Cek distribusi\n",
    "print(\"Distribusi label otomatis:\")\n",
    "print(data_sosmed_final[\"label\"].value_counts())\n",
    "\n",
    "# Preview\n",
    "print(\"\\nContoh data hasil labeling:\")\n",
    "print(data_sosmed_final.head(10))\n",
    "\n",
    "# Simpan ke CSV untuk koreksi manual\n",
    "data_sosmed_final.to_csv(\"sosmed_labeled_auto.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\n‚úÖ Dataset sosmed dengan kolom [text, label, sumber] berhasil disimpan ke 'sosmed_labeled_auto.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ffe48d-b794-4033-b807-7c3509078027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi label (0=REAL, 1=HOAX):\n",
      "label\n",
      "0    23081\n",
      "1      563\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Contoh Tweet REAL ===\n",
      "0    Ketahui informasi pembagian #PPKM di wilayah J...\n",
      "1    Tempat Ibadah di Wilayah PPKM Level 1 Boleh Be...\n",
      "2    Juru bicara Satgas Covid-19, Wiku Adisasmito m...\n",
      "3    Ketahui informasi pembagian #PPKM di wilayah J...\n",
      "4    Kementerian Agama menerbitkan Surat Edaran Nom...\n",
      "Name: text, dtype: object\n",
      "\n",
      "=== Contoh Tweet HOAX ===\n",
      "41     ‚ÄúSanksi ini diberikan agar mereka tidak melang...\n",
      "52     Ye ampun mbh anda seorang Wapres loh.&amp;Gela...\n",
      "98     Mereka bermain main dengan emosi rakyatnya . P...\n",
      "222    Pesan akhir Pak Menko Luhut B. Pandjaitan ke #...\n",
      "259    Kebijakan PPKM tahun 2021 lalu cukup mengejutk...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell: Labeling Hoax Sosial Media =====\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset asli\n",
    "df = pd.read_csv(\"INA_TweetsPPKM_Labeled_Pure.csv\", sep=\"\\t\")\n",
    "\n",
    "# Standarisasi kolom\n",
    "data_sosmed = pd.DataFrame({\n",
    "    \"text\": df[\"Tweet\"].astype(str),\n",
    "})\n",
    "\n",
    "# Daftar kata/kalimat kunci hoax\n",
    "hoax_keywords = [\n",
    "    \"terbukti\",\n",
    "    \"ampuh\",\n",
    "    \"langsung sembuh\",\n",
    "    \"terungkap\",\n",
    "    \"awas\",\n",
    "    \"hati-hati\",\n",
    "    \"jangan sampai viral\",\n",
    "    \"segera sebarkan\",\n",
    "    \"mereka\",\n",
    "    \"oknum\",\n",
    "    \"pihak tertentu\",\n",
    "    \"berdasarkan data rahasia\",\n",
    "    \"seorang ahli mengatakan\",\n",
    "    \"pesan berantai dari grup whatsapp\"\n",
    "]\n",
    "\n",
    "# Fungsi deteksi hoax berdasarkan keywords\n",
    "def label_rule(text):\n",
    "    text = text.lower()\n",
    "    for kw in hoax_keywords:\n",
    "        if kw in text:\n",
    "            return 1   # HOAX\n",
    "    return 0           # REAL\n",
    "\n",
    "# Terapkan aturan labeling\n",
    "data_sosmed[\"label\"] = data_sosmed[\"text\"].apply(label_rule)\n",
    "\n",
    "# Tambahin kolom sumber\n",
    "data_sosmed[\"sumber\"] = \"Twitter\"\n",
    "\n",
    "# Lihat hasil distribusi\n",
    "print(\"Distribusi label (0=REAL, 1=HOAX):\")\n",
    "print(data_sosmed[\"label\"].value_counts())\n",
    "\n",
    "# Contoh hasil: 5 REAL + 5 HOAX\n",
    "print(\"\\n=== Contoh Tweet REAL ===\")\n",
    "print(data_sosmed[data_sosmed[\"label\"] == 0][\"text\"].head(5))\n",
    "\n",
    "print(\"\\n=== Contoh Tweet HOAX ===\")\n",
    "print(data_sosmed[data_sosmed[\"label\"] == 1][\"text\"].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6902a17-bb9b-4111-ad25-bbda08f90fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi awal (sebelum balancing):\n",
      "label\n",
      "0    23081\n",
      "1      563\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribusi setelah undersample:\n",
      "label\n",
      "0    563\n",
      "1    563\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell Tambahan: Menangani Imbalance Dataset =====\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(\"Distribusi awal (sebelum balancing):\")\n",
    "print(data_sosmed[\"label\"].value_counts())\n",
    "\n",
    "# ==== Opsi 1: Undersampling (kurangi kelas mayoritas) ====\n",
    "def undersample(df):\n",
    "    df_majority = df[df.label == 0]  # REAL\n",
    "    df_minority = df[df.label == 1]  # HOAX\n",
    "    \n",
    "    # Samain jumlah REAL dengan HOAX\n",
    "    df_majority_downsampled = resample(\n",
    "        df_majority,\n",
    "        replace=False,      # tanpa pengulangan\n",
    "        n_samples=len(df_minority),\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "    return df_balanced\n",
    "\n",
    "# ==== Opsi 2: Oversampling SMOTE (nambah data HOAX) ====\n",
    "def oversample_smote(X, y):\n",
    "    sm = SMOTE(random_state=42, sampling_strategy='auto')\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    return X_res, y_res\n",
    "\n",
    "# Contoh cara pakai:\n",
    "# 1) Kalau mau pakai undersample (langsung dataframe siap pakai):\n",
    "data_sosmed_bal = undersample(data_sosmed)\n",
    "print(\"\\nDistribusi setelah undersample:\")\n",
    "print(data_sosmed_bal[\"label\"].value_counts())\n",
    "\n",
    "# 2) Kalau mau pakai SMOTE (setelah vectorization, nanti di Cell 4):\n",
    "# X_res, y_res = oversample_smote(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e187a04-4377-4570-b21e-18d33d9c715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset sosmed balanced berhasil disimpan -> dataset_sosmed_hoax.csv\n",
      "label\n",
      "0    563\n",
      "1    563\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset asli (ubah sesuai nama file lo)\n",
    "df = pd.read_csv(\"INA_TweetsPPKM_Labeled_Pure.csv\", sep=\"\\t\")\n",
    "\n",
    "# Standarisasi kolom (biar ada text, label, sumber)\n",
    "df = df.rename(columns={\"Tweet\": \"text\"})\n",
    "\n",
    "# Daftar kata kunci indikasi hoax\n",
    "hoax_keywords = [\n",
    "    \"terbukti\", \"ampuh\", \"langsung sembuh\", \"terungkap\", \"awas\",\n",
    "    \"hati-hati\", \"jangan sampai viral\", \"segera sebarkan\",\n",
    "    \"mereka\", \"oknum\", \"pihak tertentu\", \"berdasarkan data rahasia\",\n",
    "    \"seorang ahli mengatakan\", \"pesan berantai dari grup whatsapp\"\n",
    "]\n",
    "\n",
    "def label_rule(text):\n",
    "    t = str(text).lower()\n",
    "    for kw in hoax_keywords:\n",
    "        if kw in t:\n",
    "            return 1   # HOAX\n",
    "    return 0           # REAL\n",
    "\n",
    "# Buat dataframe baru\n",
    "data_sosmed = pd.DataFrame({\n",
    "    \"text\": df[\"text\"].fillna(\"\"),\n",
    "    \"label\": df[\"text\"].apply(label_rule),\n",
    "    \"sumber\": \"Twitter\"\n",
    "})\n",
    "\n",
    "# Balance dataset: samakan jumlah HOAX & REAL\n",
    "min_count = data_sosmed[\"label\"].value_counts().min()\n",
    "data_sosmed_balanced = pd.concat([\n",
    "    data_sosmed[data_sosmed[\"label\"] == 0].sample(min_count, random_state=42),\n",
    "    data_sosmed[data_sosmed[\"label\"] == 1].sample(min_count, random_state=42)\n",
    "], ignore_index=True)\n",
    "\n",
    "# Simpan ke CSV\n",
    "data_sosmed_balanced.to_csv(\"dataset_sosmed_hoax.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset sosmed balanced berhasil disimpan -> dataset_sosmed_hoax.csv\")\n",
    "print(data_sosmed_balanced[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb46b977-42a6-485c-a624-5b2ade574992",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_sosmed_hoax.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m data_sosmed_balanced = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_sosmed_hoax.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset sosmed loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(data_sosmed_balanced.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dataset_sosmed_hoax.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_sosmed_balanced = pd.read_csv(\"dataset_sosmed_hoax.csv\")\n",
    "print(\"‚úÖ Dataset sosmed loaded!\")\n",
    "print(data_sosmed_balanced.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e73726-10ae-4f63-8db3-3d988bbb36c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
