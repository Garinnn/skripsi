{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66e6750-de04-46e8-8739-791ec8e12115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.6\n",
      "OS: Windows 11\n",
      "pandas: 2.2.2\n",
      "numpy : 2.1.1\n",
      "scikit-learn: 1.5.2\n",
      "imbalanced-learn: 0.14.0\n",
      "Sastrawi: terinstal\n",
      "tqdm: terinstal\n",
      "joblib: 1.4.2\n",
      "\n",
      "Working directory: C:\\Users\\muham\\Skripsi\n",
      "\n",
      "Jumlah file CSV/XLSX di folder: 5\n",
      " - dataset_cnn_10k_cleaned.csv\n",
      " - dataset_kompas_4k_cleaned.csv\n",
      " - dataset_sosmed_hoax.csv\n",
      " - dataset_tempo_6k_cleaned.csv\n",
      " - dataset_turnbackhoax_10_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: Setup & Cek Environment =====\n",
    "import os, sys\n",
    "import platform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cek versi paket penting (untuk debugging)\n",
    "def version_str(pkg):\n",
    "    try:\n",
    "        return pkg.__version__\n",
    "    except Exception:\n",
    "        return \"tidak tersedia\"\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"OS:\", platform.system(), platform.release())\n",
    "print(\"pandas:\", version_str(pd))\n",
    "print(\"numpy :\", version_str(np))\n",
    "\n",
    "# Cek scikit-learn, imbalanced-learn, Sastrawi, tqdm, joblib\n",
    "sklearn_v = \"tidak tersedia\"\n",
    "imblearn_v = \"tidak tersedia\"\n",
    "sastrawi_v = \"tidak tersedia\"\n",
    "tqdm_v = \"tidak tersedia\"\n",
    "joblib_v = \"tidak tersedia\"\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    sklearn_v = sklearn.__version__\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import imblearn\n",
    "    imblearn_v = imblearn.__version__\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import Sastrawi\n",
    "    sastrawi_v = \"terinstal\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    tqdm_v = \"terinstal\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "    joblib_v = joblib.__version__\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"scikit-learn:\", sklearn_v)\n",
    "print(\"imbalanced-learn:\", imblearn_v)\n",
    "print(\"Sastrawi:\", sastrawi_v)\n",
    "print(\"tqdm:\", tqdm_v)\n",
    "print(\"joblib:\", joblib_v)\n",
    "\n",
    "# Tampilkan working dir dan daftar file CSV/XLSX\n",
    "cwd = os.getcwd()\n",
    "print(\"\\nWorking directory:\", cwd)\n",
    "files = os.listdir(cwd)\n",
    "csv_files = [f for f in files if f.lower().endswith((\".csv\",\".xlsx\"))]\n",
    "\n",
    "print(f\"\\nJumlah file CSV/XLSX di folder: {len(csv_files)}\")\n",
    "for f in csv_files:\n",
    "    print(\" -\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "452e627d-a183-4730-be6d-33cdca8ba3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran dataset berita: (31353, 3)\n",
      "\n",
      "Distribusi label (0=REAL, 1=HOAX):\n",
      "label\n",
      "0    20972\n",
      "1    10381\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Contoh data gabungan:\n",
      "                                                    text  label        sumber\n",
      "24617  Hasil Periksa Fakta Renanda Dwina Putri (Anggo...      1  Turnbackhoax\n",
      "26750  Pernyataan bahwa virus Corona baru penyebab Co...      1  Turnbackhoax\n",
      "10558  Mengapa Hoaks dan Isu PKI Masih Laku untuk Pro...      0        Kompas\n",
      "7570   Menag: Umrah Desember Jadi Uji Coba untuk Buka...      0           CNN\n",
      "28119  Screenshot berita yang berjudul ‚ÄúKALAU SAYA ME...      1  Turnbackhoax\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2a: Load & Gabungkan Dataset Berita =====\n",
    "import pandas as pd\n",
    "\n",
    "# Baca dataset berita\n",
    "data1 = pd.read_csv(\"dataset_cnn_10k_cleaned.csv\")\n",
    "data2 = pd.read_csv(\"dataset_kompas_4k_cleaned.csv\")\n",
    "data3 = pd.read_csv(\"dataset_tempo_6k_cleaned.csv\")\n",
    "data4 = pd.read_csv(\"dataset_turnbackhoax_10_cleaned.csv\")\n",
    "\n",
    "# Fungsi standarisasi dataset berita\n",
    "def standardize_dataset(df, sumber=\"unknown\"):\n",
    "    # Pilih teks utama\n",
    "    if \"text_new\" in df.columns:\n",
    "        teks = df[\"text_new\"].fillna(\"\")\n",
    "    elif \"FullText\" in df.columns:\n",
    "        teks = df[\"FullText\"].fillna(\"\")\n",
    "    elif \"Narasi\" in df.columns:\n",
    "        teks = df[\"Narasi\"].fillna(\"\")\n",
    "    elif \"text\" in df.columns:\n",
    "        teks = df[\"text\"].fillna(\"\")\n",
    "    else:\n",
    "        raise ValueError(f\"Tidak ada kolom teks valid di dataset {sumber}\")\n",
    "    \n",
    "    # Pilih label\n",
    "    if \"hoax\" in df.columns:\n",
    "        label = df[\"hoax\"].astype(int)\n",
    "    elif \"label\" in df.columns:\n",
    "        label = df[\"label\"].astype(int)\n",
    "    else:\n",
    "        raise ValueError(f\"Tidak ada kolom label di dataset {sumber}\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"text\": teks,\n",
    "        \"label\": label,\n",
    "        \"sumber\": sumber\n",
    "    })\n",
    "\n",
    "# Standarisasi semua dataset\n",
    "datasets_berita = []\n",
    "datasets_berita.append(standardize_dataset(data1, \"CNN\"))\n",
    "datasets_berita.append(standardize_dataset(data2, \"Kompas\"))\n",
    "datasets_berita.append(standardize_dataset(data3, \"Tempo\"))\n",
    "datasets_berita.append(standardize_dataset(data4, \"Turnbackhoax\"))\n",
    "\n",
    "# Gabungkan\n",
    "data_berita = pd.concat(datasets_berita, ignore_index=True)\n",
    "\n",
    "# Info dataset gabungan\n",
    "print(\"Ukuran dataset berita:\", data_berita.shape)\n",
    "print(\"\\nDistribusi label (0=REAL, 1=HOAX):\")\n",
    "print(data_berita[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nContoh data gabungan:\")\n",
    "print(data_berita.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151270b8-c404-415e-9206-43f739a3c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ukuran dataset sosmed: (1126, 3)\n",
      "\n",
      "Distribusi label (0=REAL, 1=HOAX):\n",
      "label\n",
      "0    563\n",
      "1    563\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Contoh data sosmed:\n",
      "                                                  text  label   sumber\n",
      "921  Manusia tidak akan memahami satu sama lain,seb...      1  Twitter\n",
      "206  Ada yang diperpanjang tapi bukan extra time #ppkm      0  Twitter\n",
      "99     Hiburan dulu guys #ppkm https://t.co/OVfw4jB530      0  Twitter\n",
      "397  Diselundupkan saat PPKM Darurat, Rokok Bodong ...      0  Twitter\n",
      "421  Halo-halo Jakarta @DKIJakarta &amp; Pak Gub @a...      0  Twitter\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2b: Load & Standarisasi Dataset Sosmed =====\n",
    "import pandas as pd\n",
    "\n",
    "# Baca dataset sosmed\n",
    "data_sosmed_raw = pd.read_csv(\"dataset_sosmed_hoax.csv\")\n",
    "\n",
    "# Fungsi standarisasi dataset sosmed\n",
    "def standardize_sosmed(df, sumber=\"Twitter\"):\n",
    "    # Pilih teks utama\n",
    "    if \"text\" in df.columns:\n",
    "        teks = df[\"text\"].fillna(\"\")\n",
    "    else:\n",
    "        raise ValueError(\"‚ö†Ô∏è Tidak ada kolom 'text' di dataset sosmed\")\n",
    "\n",
    "    # Pastikan label ada\n",
    "    if \"label\" in df.columns:\n",
    "        label = df[\"label\"].astype(int)\n",
    "    else:\n",
    "        raise ValueError(\"‚ö†Ô∏è Tidak ada kolom 'label' di dataset sosmed\")\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"text\": teks,\n",
    "        \"label\": label,\n",
    "        \"sumber\": sumber\n",
    "    })\n",
    "\n",
    "# Standarisasi dataset sosmed\n",
    "data_sosmed = standardize_sosmed(data_sosmed_raw)\n",
    "\n",
    "# Info dataset sosmed\n",
    "print(\"Ukuran dataset sosmed:\", data_sosmed.shape)\n",
    "print(\"\\nDistribusi label (0=REAL, 1=HOAX):\")\n",
    "print(data_sosmed[\"label\"].value_counts())\n",
    "\n",
    "print(\"\\nContoh data sosmed:\")\n",
    "print(data_sosmed.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3770cc54-3754-49cf-a707-a41fdd5c5976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Preprocessing dataset berita...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [3:49:14<00:00, 859.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Preprocessing dataset sosmed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:05<00:00, 125.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing selesai!\n",
      "\n",
      "Contoh hasil preprocessing data berita:\n",
      "                                                text  \\\n",
      "0  Anies di Milad BKMT: Pengajian Menghasilkan Ib...   \n",
      "1  Edy Soal Pilgub Sumut: Kalau yang Maju Abal-ab...   \n",
      "2  PKB Bakal Daftarkan Menaker Ida Fauziyah Jadi ...   \n",
      "3  Gede Pasek Doakan AHY Jadi Capres atau Cawapre...   \n",
      "4  PKN Siapkan Jabatan Khusus Buat Anas Urbaningr...   \n",
      "\n",
      "                                          clean_text  label  \n",
      "0  anies milad bkmt aji hasil ibu ibu tahu mantan...      0  \n",
      "1  edy soal pilgub sumut kalau maju abal abal pak...      0  \n",
      "2  pkb bakal daftar menaker ida fauziyah jadi cal...      0  \n",
      "3  gede pasek doa ahy jadi capres cawapres ketua ...      0  \n",
      "4  pkn siap jabat khusus buat anas urbaningrum us...      0  \n",
      "\n",
      "Contoh hasil preprocessing data sosmed:\n",
      "                                                text  \\\n",
      "0  Aturan 20 Menit Makan di Tempat Tak Terpantau ...   \n",
      "1       #BeritaTerkini #PPKM https://t.co/sPOewNypIu   \n",
      "2  Laju penyebaran Covid turun. Dengan begitu, ta...   \n",
      "3  @rbkunwas\\n#indonesia #imigrasi #imigrasiindon...   \n",
      "4  Persiapan 2024 üëéüëéüëéüëékacauuu\\n#PPKM #IkatanCinta...   \n",
      "\n",
      "                                          clean_text  label  \n",
      "0  atur menit makan tempat tak pantau ppkm purwok...      0  \n",
      "1                                 beritaterkini ppkm      0  \n",
      "2  laju sebar covid turun tanggal juli bakal rela...      0  \n",
      "3  rbkunwas indonesia imigrasi imigrasiindonesia ...      0  \n",
      "4  siap kacauuu ppkm ikatancintaep ikatancintaep ...      0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: Preprocessing untuk Berita & Sosmed =====\n",
    "import re\n",
    "import pandas as pd\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Setup stemmer & stopwords\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "stop_factory = StopWordRemoverFactory()\n",
    "extra_stopwords = [\"dengan\",\"bahwa\",\"karena\",\"sudah\",\"juga\",\"akan\",\"untuk\"]\n",
    "stop_words = set(stop_factory.get_stop_words() + extra_stopwords)\n",
    "\n",
    "# Cache buat stemming (biar lebih cepat, ga ulang-ulang)\n",
    "stem_cache = {}\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = text.lower()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\", \" \", s)   # hapus URL\n",
    "    s = re.sub(r\"[^a-zA-Z\\s]\", \" \", s)      # hapus angka & simbol\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()      # hapus spasi ganda\n",
    "    \n",
    "    tokens = [w for w in s.split() if w not in stop_words]\n",
    "    tokens = [stem_cache[w] if w in stem_cache else stemmer.stem(w) for w in tokens]\n",
    "    for w in tokens:\n",
    "        if w not in stem_cache:\n",
    "            stem_cache[w] = stemmer.stem(w)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Fungsi apply per chunk (biar ga berat kalau data banyak)\n",
    "def apply_in_chunks(series, func, chunk=2000):\n",
    "    out_chunks = []\n",
    "    for i in tqdm(range(0, len(series), chunk)):\n",
    "        out = series.iloc[i:i+chunk].apply(func)\n",
    "        out_chunks.append(out)\n",
    "    return pd.concat(out_chunks)\n",
    "\n",
    "# Preprocessing data berita\n",
    "print(\"‚ö° Preprocessing dataset berita...\")\n",
    "data_berita['clean_text'] = apply_in_chunks(data_berita['text'].astype(str), clean_text, chunk=2000)\n",
    "\n",
    "# Preprocessing data sosmed\n",
    "print(\"‚ö° Preprocessing dataset sosmed...\")\n",
    "data_sosmed['clean_text'] = apply_in_chunks(data_sosmed['text'].astype(str), clean_text, chunk=2000)\n",
    "\n",
    "print(\"‚úÖ Preprocessing selesai!\")\n",
    "print(\"\\nContoh hasil preprocessing data berita:\")\n",
    "print(data_berita[['text','clean_text','label']].head())\n",
    "\n",
    "print(\"\\nContoh hasil preprocessing data sosmed:\")\n",
    "print(data_sosmed[['text','clean_text','label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532f521a-1ac4-40eb-b954-40e158b44575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dataset Berita ===\n",
      "Shape X_train: (25082, 20000)\n",
      "Shape X_test : (6271, 20000)\n",
      "Distribusi train: {np.int64(0): np.int64(16777), np.int64(1): np.int64(8305)}\n",
      "Distribusi test : {np.int64(0): np.int64(4195), np.int64(1): np.int64(2076)}\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4a: Vectorization & Split untuk Dataset Berita =====\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vectorizer TF-IDF untuk berita\n",
    "vectorizer_berita = TfidfVectorizer(\n",
    "    ngram_range=(1,2),   # unigram + bigram\n",
    "    max_features=20000,  # ambil max 20k fitur\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Pakai clean_text dari Cell 3\n",
    "X_berita = vectorizer_berita.fit_transform(data_berita[\"clean_text\"])\n",
    "y_berita = data_berita[\"label\"].values\n",
    "\n",
    "# Ambil nama fitur (kata & bigram)\n",
    "feature_names = np.array(vectorizer_berita.get_feature_names_out())\n",
    "\n",
    "# Hitung rata-rata bobot TF-IDF tiap fitur\n",
    "tfidf_mean = X_berita.mean(axis=0).A1  # ubah ke array 1D\n",
    "\n",
    "# Ambil 10 fitur dengan nilai TF-IDF tertinggi\n",
    "top_n = 10\n",
    "top_idx = np.argsort(tfidf_mean)[-top_n:][::-1]\n",
    "\n",
    "# Buat tabel\n",
    "tfidf_table = pd.DataFrame({\n",
    "    \"Kata / Frasa\": feature_names[top_idx],\n",
    "    \"Nilai TF-IDF\": tfidf_mean[top_idx]\n",
    "})\n",
    "\n",
    "tfidf_table\n",
    "\n",
    "\n",
    "# Split train-test\n",
    "Xb_train, Xb_test, yb_train, yb_test = train_test_split(\n",
    "    X_berita, y_berita, test_size=0.2, random_state=42, stratify=y_berita\n",
    ")\n",
    "\n",
    "print(\"=== Dataset Berita ===\")\n",
    "print(\"Shape X_train:\", Xb_train.shape)\n",
    "print(\"Shape X_test :\", Xb_test.shape)\n",
    "print(\"Distribusi train:\", dict(zip(*np.unique(yb_train, return_counts=True))))\n",
    "print(\"Distribusi test :\", dict(zip(*np.unique(yb_test, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f6d340-9f00-4999-8ed3-503544ffffe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kata / Frasa</th>\n",
       "      <th>Nilai TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sebut</td>\n",
       "      <td>0.022581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kata</td>\n",
       "      <td>0.021812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>partai</td>\n",
       "      <td>0.019775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jadi</td>\n",
       "      <td>0.019381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politik</td>\n",
       "      <td>0.017351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jokowi</td>\n",
       "      <td>0.016882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>presiden</td>\n",
       "      <td>0.016825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>indonesia</td>\n",
       "      <td>0.015083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jelas</td>\n",
       "      <td>0.014453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>laku</td>\n",
       "      <td>0.014391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Kata / Frasa  Nilai TF-IDF\n",
       "0        sebut      0.022581\n",
       "1         kata      0.021812\n",
       "2       partai      0.019775\n",
       "3         jadi      0.019381\n",
       "4      politik      0.017351\n",
       "5       jokowi      0.016882\n",
       "6     presiden      0.016825\n",
       "7    indonesia      0.015083\n",
       "8        jelas      0.014453\n",
       "9         laku      0.014391"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 4a.1: Tabel Contoh Hasil TF-IDF =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Ambil nama fitur (kata & bigram)\n",
    "feature_names = np.array(vectorizer_berita.get_feature_names_out())\n",
    "\n",
    "# Hitung rata-rata bobot TF-IDF tiap fitur\n",
    "tfidf_mean = X_berita.mean(axis=0).A1  # konversi sparse ‚Üí array 1D\n",
    "\n",
    "# Tentukan jumlah kata yang ingin ditampilkan\n",
    "top_n = 10\n",
    "\n",
    "# Ambil indeks TF-IDF tertinggi\n",
    "top_idx = np.argsort(tfidf_mean)[-top_n:][::-1]\n",
    "\n",
    "# Buat tabel\n",
    "tfidf_table = pd.DataFrame({\n",
    "    \"Kata / Frasa\": feature_names[top_idx],\n",
    "    \"Nilai TF-IDF\": tfidf_mean[top_idx]\n",
    "})\n",
    "\n",
    "# Tampilkan tabel (WAJIB pakai display)\n",
    "display(tfidf_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87395a0f-5c62-4c30-8d7b-87f6d58a3698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluasi Model (MNB - Berita) ===\n",
      "Akurasi: 0.9811832243661298\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        REAL       0.99      0.99      0.99      4195\n",
      "        HOAX       0.97      0.97      0.97      2076\n",
      "\n",
      "    accuracy                           0.98      6271\n",
      "   macro avg       0.98      0.98      0.98      6271\n",
      "weighted avg       0.98      0.98      0.98      6271\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4137   58]\n",
      " [  60 2016]]\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5a: Training Multinomial Naive Bayes untuk Berita =====\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Inisialisasi model\n",
    "mnb_berita = MultinomialNB()\n",
    "\n",
    "# Training\n",
    "mnb_berita.fit(Xb_train, yb_train)\n",
    "\n",
    "# Prediksi di data test\n",
    "yb_pred = mnb_berita.predict(Xb_test)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"=== Evaluasi Model (MNB - Berita) ===\")\n",
    "print(\"Akurasi:\", accuracy_score(yb_test, yb_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(yb_test, yb_pred, target_names=[\"REAL\",\"HOAX\"]))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(yb_test, yb_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0ae885e-05ea-4dc6-84b8-ac820b5126d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kata</th>\n",
       "      <th>P(Kata | Real)</th>\n",
       "      <th>P(Kata | Hoax)</th>\n",
       "      <th>Selisih (Hoax - Real)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>partai</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>-0.002376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politik</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>-0.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kata</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>-0.001602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milu</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-0.001549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ketua</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>-0.001456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>narasi</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.002174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>referensi</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.002057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>konten</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.002051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kategori</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.002026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sumber</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Kata  P(Kata | Real)  P(Kata | Hoax)  Selisih (Hoax - Real)\n",
       "0     partai        0.002526        0.000150              -0.002376\n",
       "1    politik        0.002186        0.000178              -0.002009\n",
       "2       kata        0.002395        0.000793              -0.001602\n",
       "3       milu        0.001656        0.000107              -0.001549\n",
       "4      ketua        0.001705        0.000250              -0.001456\n",
       "5     narasi        0.000055        0.002229               0.002174\n",
       "6  referensi        0.000017        0.002074               0.002057\n",
       "7     konten        0.000035        0.002086               0.002051\n",
       "8   kategori        0.000039        0.002065               0.002026\n",
       "9     sumber        0.000108        0.002118               0.002010"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 5a.1: Analisis Probabilitas Kata per Kelas (MNB - Berita) =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ambil nama fitur dari TF-IDF\n",
    "feature_names = np.array(vectorizer_berita.get_feature_names_out())\n",
    "\n",
    "# Ambil probabilitas log P(kata|kelas)\n",
    "log_prob = mnb_berita.feature_log_prob_\n",
    "\n",
    "# Ubah ke probabilitas asli\n",
    "prob = np.exp(log_prob)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_prob = pd.DataFrame({\n",
    "    \"Kata\": feature_names,\n",
    "    \"P(Kata | Real)\": prob[0],\n",
    "    \"P(Kata | Hoax)\": prob[1]\n",
    "})\n",
    "\n",
    "# Tambahkan selisih probabilitas\n",
    "df_prob[\"Selisih (Hoax - Real)\"] = df_prob[\"P(Kata | Hoax)\"] - df_prob[\"P(Kata | Real)\"]\n",
    "\n",
    "# Ambil 10 kata paling khas Hoax & Real\n",
    "top_hoax = df_prob.sort_values(\"Selisih (Hoax - Real)\", ascending=False).head(5)\n",
    "top_real = df_prob.sort_values(\"Selisih (Hoax - Real)\").head(5)\n",
    "\n",
    "# Gabungkan\n",
    "tabel_mnb = pd.concat([top_real, top_hoax]).reset_index(drop=True)\n",
    "\n",
    "tabel_mnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9719c45-0b3a-4ce6-8232-1fdfdf56637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Xs_train: (900, 19466)\n",
      "Shape Xs_test : (226, 19466)\n",
      "Distribusi train: {np.int64(0): np.int64(450), np.int64(1): np.int64(450)}\n",
      "Distribusi test : {np.int64(0): np.int64(113), np.int64(1): np.int64(113)}\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4b: Vectorization & Split untuk Sosmed =====\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Vectorizer TF-IDF untuk sosmed\n",
    "vectorizer_sosmed = TfidfVectorizer(\n",
    "    ngram_range=(1,2),   # unigram + bigram\n",
    "    max_features=20000,  # max 20k fitur\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Transformasi teks sosmed\n",
    "Xs = vectorizer_sosmed.fit_transform(data_sosmed[\"clean_text\"])\n",
    "ys = data_sosmed[\"label\"].values\n",
    "\n",
    "# Split train-test\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(\n",
    "    Xs, ys, test_size=0.2, random_state=42, stratify=ys\n",
    ")\n",
    "\n",
    "print(\"Shape Xs_train:\", Xs_train.shape)\n",
    "print(\"Shape Xs_test :\", Xs_test.shape)\n",
    "print(\"Distribusi train:\", dict(zip(*np.unique(ys_train, return_counts=True))))\n",
    "print(\"Distribusi test :\", dict(zip(*np.unique(ys_test, return_counts=True))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee924ce2-9f4d-4e7f-9d87-b57f549a49c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluasi Model Sosmed (MNB) ===\n",
      "Akurasi: 0.8141592920353983\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        REAL       0.83      0.79      0.81       113\n",
      "        HOAX       0.80      0.84      0.82       113\n",
      "\n",
      "    accuracy                           0.81       226\n",
      "   macro avg       0.82      0.81      0.81       226\n",
      "weighted avg       0.82      0.81      0.81       226\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[89 24]\n",
      " [18 95]]\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5b: Training Multinomial Naive Bayes untuk Sosmed =====\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Inisialisasi model MNB khusus sosmed\n",
    "mnb_sosmed = MultinomialNB()\n",
    "\n",
    "# Training\n",
    "mnb_sosmed.fit(Xs_train, ys_train)\n",
    "\n",
    "# Prediksi di data test\n",
    "ys_pred = mnb_sosmed.predict(Xs_test)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"=== Evaluasi Model Sosmed (MNB) ===\")\n",
    "print(\"Akurasi:\", accuracy_score(ys_test, ys_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(ys_test, ys_pred, target_names=[\"REAL\",\"HOAX\"]))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(ys_test, ys_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c500ee5-898e-414d-9de6-5cd45c2accb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∞ [BERITA]\n",
      "Teks (potongan): Presiden Joko Widodo meresmikan proyek kereta cepat Jakarta‚ÄìBandung. ...\n",
      "Probabilitas => REAL: 0.7775386911818677  | HOAX: 0.22246130881812903\n",
      "Prediksi: REAL ‚úÖ\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'REAL'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 6a: Fungsi Prediksi Berita =====\n",
    "\n",
    "def prediksi_berita(teks, model=mnb_berita, vec=vectorizer_berita):\n",
    "    teks_clean = clean_text(teks)\n",
    "    fitur = vec.transform([teks_clean])\n",
    "    \n",
    "    probas = model.predict_proba(fitur)[0]\n",
    "    pred = model.predict(fitur)[0]\n",
    "    \n",
    "    print(\"üì∞ [BERITA]\")\n",
    "    print(\"Teks (potongan):\", teks[:120], \"...\")\n",
    "    print(\"Probabilitas => REAL:\", probas[0], \" | HOAX:\", probas[1])\n",
    "    print(\"Prediksi:\", \"HOAX ‚ùå\" if pred == 1 else \"REAL ‚úÖ\")\n",
    "    print(\"-\"*80)\n",
    "    return \"HOAX\" if pred == 1 else \"REAL\"\n",
    "\n",
    "# Contoh tes berita\n",
    "prediksi_berita(\"Presiden Joko Widodo meresmikan proyek kereta cepat Jakarta‚ÄìBandung.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c0e758f-15d2-41a4-a56d-64339e1270e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì± [SOSMED]\n",
      "Teks (potongan): Minum air rebusan kabel listrik terbukti ampuh menyembuhkan segala penyakit, segera sebarkan! ...\n",
      "Probabilitas => REAL: 0.3772036056280858  | HOAX: 0.6227963943719159\n",
      "Prediksi: HOAX ‚ùå\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HOAX'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Cell 6b: Fungsi Prediksi Sosmed =====\n",
    "\n",
    "def prediksi_sosmed(teks, model=mnb_sosmed, vec=vectorizer_sosmed):\n",
    "    teks_clean = clean_text(teks)\n",
    "    fitur = vec.transform([teks_clean])\n",
    "    \n",
    "    probas = model.predict_proba(fitur)[0]\n",
    "    pred = model.predict(fitur)[0]\n",
    "    \n",
    "    print(\"üì± [SOSMED]\")\n",
    "    print(\"Teks (potongan):\", teks[:120], \"...\")\n",
    "    print(\"Probabilitas => REAL:\", probas[0], \" | HOAX:\", probas[1])\n",
    "    print(\"Prediksi:\", \"HOAX ‚ùå\" if pred == 1 else \"REAL ‚úÖ\")\n",
    "    print(\"-\"*80)\n",
    "    return \"HOAX\" if pred == 1 else \"REAL\"\n",
    "\n",
    "# Contoh tes sosmed\n",
    "prediksi_sosmed(\"Minum air rebusan kabel listrik terbukti ampuh menyembuhkan segala penyakit, segera sebarkan!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18d5a92f-6ff8-4ff0-a964-26198fe94dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semua model & vectorizer berhasil disimpan!\n",
      " - model_hoax_berita.pkl\n",
      " - tfidf_vectorizer_berita.pkl\n",
      " - model_hoax_sosmed.pkl\n",
      " - tfidf_vectorizer_sosmed.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: Simpan Model & Vectorizer =====\n",
    "import joblib\n",
    "\n",
    "# Simpan model berita & vectorizer\n",
    "joblib.dump(mnb_berita, \"model_hoax_berita.pkl\")\n",
    "joblib.dump(vectorizer_berita, \"tfidf_vectorizer_berita.pkl\")\n",
    "\n",
    "# Simpan model sosmed & vectorizer\n",
    "joblib.dump(mnb_sosmed, \"model_hoax_sosmed.pkl\")\n",
    "joblib.dump(vectorizer_sosmed, \"tfidf_vectorizer_sosmed.pkl\")\n",
    "\n",
    "print(\"‚úÖ Semua model & vectorizer berhasil disimpan!\")\n",
    "print(\" - model_hoax_berita.pkl\")\n",
    "print(\" - tfidf_vectorizer_berita.pkl\")\n",
    "print(\" - model_hoax_sosmed.pkl\")\n",
    "print(\" - tfidf_vectorizer_sosmed.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd86c8e-8200-4ea1-ae75-385b89e1b2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Semua model & vectorizer berhasil diload kembali!\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8: Load Model & Vectorizer =====\n",
    "import joblib\n",
    "\n",
    "# Load model & vectorizer berita\n",
    "mnb_berita_loaded = joblib.load(\"model_hoax_berita.pkl\")\n",
    "vectorizer_berita_loaded = joblib.load(\"tfidf_vectorizer_berita.pkl\")\n",
    "\n",
    "# Load model & vectorizer sosmed\n",
    "mnb_sosmed_loaded = joblib.load(\"model_hoax_sosmed.pkl\")\n",
    "vectorizer_sosmed_loaded = joblib.load(\"tfidf_vectorizer_sosmed.pkl\")\n",
    "\n",
    "print(\"‚úÖ Semua model & vectorizer berhasil diload kembali!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92eb075d-d1b5-43a0-8ff5-8c45ed5ebd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL ‚úÖ\n",
      "HOAX ‚ùå\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 9: Fungsi Prediksi dengan Model yang Dilooad =====\n",
    "\n",
    "def prediksi_berita_loaded(teks):\n",
    "    teks_clean = clean_text(teks)  # pakai fungsi preprocessing Cell 3\n",
    "    fitur = vectorizer_berita_loaded.transform([teks_clean])\n",
    "    pred = mnb_berita_loaded.predict(fitur)[0]\n",
    "    return \"HOAX ‚ùå\" if pred == 1 else \"REAL ‚úÖ\"\n",
    "\n",
    "def prediksi_sosmed_loaded(teks):\n",
    "    teks_clean = clean_text(teks)\n",
    "    fitur = vectorizer_sosmed_loaded.transform([teks_clean])\n",
    "    pred = mnb_sosmed_loaded.predict(fitur)[0]\n",
    "    return \"HOAX ‚ùå\" if pred == 1 else \"REAL ‚úÖ\"\n",
    "\n",
    "# Coba tes\n",
    "print(prediksi_berita_loaded(\"Presiden Jokowi meresmikan kereta cepat Jakarta-Bandung\"))\n",
    "print(prediksi_sosmed_loaded(\"awas jangan sampai viral vaksin ini langsung sembuh katanya\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b7d7e-e0d2-4b80-9f48-ba559a4d35fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
